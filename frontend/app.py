import streamlit as st
from PIL import Image
from pathlib import Path
import sys
import streamlit.components.v1 as components

# ============================================================
# ========== PATH SETUP ======================================
# ============================================================
st.set_page_config(page_title="NYS Policies Assistant", layout="wide")

ROOT = Path(__file__).resolve().parents[1]  # project root
SRC_DIR = ROOT / "src"
sys.path.append(str(SRC_DIR))  # allow import from src/

# ============================================================
# ========== IMPORT PIPELINE MODULES =========================
# ============================================================
try:
    from healthcare_rag_llm.llm.llm_client import LLMClient
    from healthcare_rag_llm.llm.response_generator import ResponseGenerator
    HAS_BACKEND = True
except Exception as e:
    HAS_BACKEND = False
    # st.warning(f"‚ö†Ô∏è Backend import failed: {e}. Mock mode enabled.")

# ============================================================
# ========== STREAMLIT PAGE CONFIG ===========================
# ============================================================
# Paths
ROOT_DIR = Path(__file__).parent
ASSETS_DIR = ROOT_DIR / "assets"
nys_logo_path = ASSETS_DIR / "nys_logo.png"

# ============================================================
# ========== HEADER ==========================================
# ============================================================
col1, col2 = st.columns([5, 1])
with col1:
    st.title("NYS Policies Assistant")
    st.caption("An AI-powered assistant for navigating New York State medical care policies.")
with col2:
    if nys_logo_path.exists():
        nys_logo = Image.open(nys_logo_path)
        st.image(nys_logo, width=200)
    else:
        st.text("NYS Logo Missing")

# ============================================================
# ========== DISCLAIMER ======================================
# ============================================================
with st.expander("üìú Disclaimer & About", expanded=True):
    st.markdown("""
    This assistant is part of the **Columbia University x KPMG Capstone Project**:
    *Intelligent Document Analysis for Healthcare Programs Using LLMs and RAG.*

    The responses are generated by an AI model referencing official
    New York State Medicaid documents and related authorities.
    They do not constitute legal advice.
    """)

# ============================================================
# ========== INITIALIZE RAG PIPELINE =========================
# ============================================================
@st.cache_resource
def load_rag_pipeline():
    if not HAS_BACKEND:
        return None
    llm_client = LLMClient(
        api_key="",  # API key
        model="gpt-5",
        provider="openai",
        base_url="https://api.bltcy.ai/v1"
    )
    return ResponseGenerator(llm_client)

rag_pipeline = load_rag_pipeline()

# ============================================================
# ========== CHAT MODE (Multiple Q&A) ========================
# ============================================================
# Initialize history
if "history" not in st.session_state:
    st.session_state["history"] = []  # [{"role": "user"/"assistant", "content": "..."}]

# Container: display history (natural order)
chat_container = st.container()

# ============================================================
# ========== DISPLAY CHAT HISTORY (layout) ===================
# ============================================================
st.markdown(
    """
    <style>
    /* container */
    .chat-container { display: flex; flex-direction: column; gap: 1rem; margin-bottom: 1rem; font-size: 15px; }

    /* chat row */
    .chat-row { display: flex; align-items: flex-start; }
    .chat-row.user { justify-content: flex-end; }

    /* chat bubble */
    .chat-bubble {
        max-width: 70%;
        padding: 0.7rem 1rem;
        border-radius: 1rem;
        line-height: 1.6;
        word-wrap: break-word;
        box-shadow: 0 1px 3px rgba(0,0,0,0.08);
    }
    .chat-bubble.assistant {
        background-color: #F4F4F4;
        border: 1px solid #E0E0E0;
        color: #000;
        border-top-left-radius: 0.3rem;
    }
    .chat-bubble.user {
        background-color: #DCF2FF;
        border: 1px solid #CDE9FF;
        color: #000;
        border-top-right-radius: 0.3rem;
    }

    /* avatar */
    .avatar {
        width: 36px; height: 36px;
        border-radius: 50%;
        display: flex; align-items: center; justify-content: center;
        font-size: 18px; font-weight: bold;
        background-color: #E5E7EB;
        margin: 0 0.5rem;
    }
    .avatar.user { background-color: #CDE9FF; }

    /* font */
    input, button, label, div[data-testid="stMarkdownContainer"] {
        font-size: 15px !important;
    }
    </style>
    """,
    unsafe_allow_html=True
)

# Load history
st.markdown('<div class="chat-container">', unsafe_allow_html=True)
for msg in st.session_state["history"]:
    role = msg["role"]
    text = msg["content"]

    if role == "assistant":
        st.markdown(
            f"""
            <div class="chat-row assistant">
                <div class="avatar assistant">ü§î</div>
                <div class="chat-bubble assistant">{text}</div>
            </div>
            """,
            unsafe_allow_html=True,
        )
    else:
        st.markdown(
            f"""
            <div class="chat-row user">
                <div class="chat-bubble user">{text}</div>
                <div class="avatar user">üë§</div>
            </div>
            """,
            unsafe_allow_html=True,
        )
st.markdown('</div>', unsafe_allow_html=True)

# ============================================================
# ========== INPUT AREA (two-line layout) ====================
# ============================================================
with st.container():
    st.markdown('<div class="input-box">', unsafe_allow_html=True)

    with st.form("chat_form", clear_on_submit=True):
        user_query = st.text_input(
            "üí¨ **Ask a Question:**",
            placeholder="e.g. When did redetermination begin for the COVID-19 Public Health Emergency unwind in New York State?",
        )

        col1, col2 = st.columns([1, 1])
        with col1:
            submitted = st.form_submit_button("üì§ Submit", use_container_width=True)
        with col2:
            cleared = st.form_submit_button("üßπ Clear Chat", use_container_width=True)

    st.markdown('</div>', unsafe_allow_html=True)

# Clear logic
if cleared:
    st.session_state["history"] = []
    st.rerun()

# Submit logic
if submitted:
    if not user_query.strip():
        st.warning("Please enter a question before submitting.")
    else:
        st.session_state["history"].append({"role": "user", "content": user_query})
        with st.spinner("Retrieving information..."):
            try:
                if rag_pipeline:
                    # --- Real backend ---
                    result = rag_pipeline.answer_question(user_query, history=st.session_state["history"])
                    answer = result.get("answer", "No answer returned.")
                    retrieved_docs = result.get("retrieved_docs", [])

                    st.success("‚úÖ Answer Retrieved")
                    st.markdown(f"**Answer:**\n\n{answer}")

                    st.markdown("### üìö Retrieved Sources")
                    if retrieved_docs:
                        for doc in retrieved_docs:
                            doc_id = doc.get("doc_id", "Unknown")
                            pages = doc.get("pages", "N/A")
                            snippet = doc.get("text", "")[:300]
                            st.markdown(f"- **{doc_id}** (pages {pages})")
                            st.caption(snippet + "...")
                    else:
                        st.caption("No source documents retrieved.")
                else:
                    # --- Mock mode ---
                    answer = (
                        "‚ö†Ô∏è Mock mode: Backend not connected.\n"
                        "No grounded answer can be generated. "
                        "Please connect the backend (RAG pipeline) to enable cited answers."
                    )
                    # st.warning("‚ö†Ô∏è Mock mode: Backend not connected.")
                    # st.markdown(
                    #     "The assistant is currently running in mock mode without a live RAG backend. "
                    #     "Please connect to the backend to generate answers."
                    # )

                st.session_state["history"].append({"role": "assistant", "content": answer})
                st.rerun()

            except Exception as e:
                st.error(f"Error: {e}")

# ============================================================
# ========== FOOTER ==========================================
# ============================================================
st.divider()
st.markdown(
    "<p style='font-size: small; color: gray;'>¬© 2025 Columbia DSI x KPMG | For educational use only.</p>",
    unsafe_allow_html=True
)